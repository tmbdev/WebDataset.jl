{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9c26b5-4e16-4c90-ac31-d4a0f8322609",
   "metadata": {},
   "source": [
    "# WebDataset\n",
    "\n",
    "WebDataset is a standard for representing dataset for training deep learning and machine learning models, with other implementation in Python [github.com/tmbdev/webdataset](http://github.com/tmbdev/webdataset) and Go [github.com/tmbdev/tarp](http://github.com/tmbdev/tarp).\n",
    "\n",
    "This is a first cut at a Julia implementation, taking advantage of Julia's multithreading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556645e-4475-414e-84e8-33d3b98ccf25",
   "metadata": {},
   "source": [
    "# Operation\n",
    "\n",
    "WebDatasets consists of a collection of tar files (\"shards\"). Each tar file is read sequentially, and multiple shards may be read in parallel.\n",
    "\n",
    "Within each tar file, files with a common basename but different extensions are grouped together into training samples. The \"basename\" is defined here as the directory name plus the file name up to the first \".\" in the file name.\n",
    "\n",
    "Training samples for learning stereo models might contain a sequence of samples of the form:\n",
    "\n",
    "```\n",
    "098953.left.jpg\n",
    "098953.right.jpg\n",
    "098953.depth.png16\n",
    "194432.left.jpg\n",
    "194432.right.jpg\n",
    "194432.depth.png16\n",
    "```\n",
    "\n",
    "Training samples are represented as `Dict{String,Any}` instances in the library.\n",
    "\n",
    "Dataset loading takes place in multiple stages:\n",
    "\n",
    "```\n",
    "tar file reading -> grouping into samples -> decoding of samples -> augmentation of samples -> general mapping -> batching -> collating\n",
    "```\n",
    "\n",
    "The `dataloader` function will set up such a loading pipeline and run it in parallel in multiple threads. The entire process is described by a dataset descriptor:\n",
    "\n",
    "```\n",
    "@with_kw mutable struct DatasetDescriptor\n",
    "    # list of shards\n",
    "    sources::Array{Union{String,Cmd}} = []\n",
    "    # size of the inline shuffle buffer\n",
    "    shuffle::Int = 1000\n",
    "    # batchsize (0 for no batching)\n",
    "    batchsize::Int = 16\n",
    "    # decoding functions\n",
    "    decoding::Array{Pair{Union{String,Regex,Renamer},Function}} = [\"\"=>x->x]\n",
    "    # augmentation functions\n",
    "    augmenting::Array{Pair{Union{String,Regex,Renamer},Function}} = [\"\"=>x->x]\n",
    "    # a general mapping function running after augmentation\n",
    "    mapping::Function = x->x\n",
    "    # collation functions running after batching\n",
    "    collating::Array{Pair{Union{String,Regex,Renamer},Function}} = [\"\"=>x->x]\n",
    "    # channel size for output channel\n",
    "    csize::Int = 100\n",
    "    # number of concurrent tasks/shards for loading\n",
    "    ntasks::Int = 4\n",
    "    # more verbose output\n",
    "    verbose::Bool = false\n",
    "    # even more verbose output\n",
    "    debug::Bool = false\n",
    "end\n",
    "```\n",
    "\n",
    "Here, `decoding`, `augmenting`, and `collating` are lists of mapping rules that are searched sequentially for a matching rule.  The default decoders are very simple:\n",
    "\n",
    "\n",
    "```\n",
    "default_decoders = [\n",
    "    \".cls\" => clsdecode,\n",
    "    Rename(r\".(jpg|jpg|png|p?m)$\", \".img\") => imdecode,\n",
    "    \".json\" => jsondecode,\n",
    "]\n",
    "```\n",
    "\n",
    "This says that every file ending in `.cls` should be decoded with the `clsdecode` function and every file ending in `.json` should be decoded using `jsondecode`. Files matching the regular expression should have their matching portion renamed to `.img` and should be decoded using the `imdecode` function.\n",
    "\n",
    "Decoder functions should take `UInt8` arrays containing the binary file content and decode into whatever data structure is desired. Augmentation functions take the decoded outputs as inputs and produce new outputs. Collating functions take a list of decoded items (e.g., all files ending in `.img`) and can, for example, stack them together into arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb11de0-aac9-4e85-b106-96be5a7ebce7",
   "metadata": {},
   "source": [
    "# Simple Example\n",
    "\n",
    "In this example, we start by defining a simple `DatasetDescriptor`; we're using the default decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094e7225-25cc-4bde-a471-5bd249584563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDescriptor\n",
       "  sources: Array{Union{Cmd, String}}((4,))\n",
       "  shuffle: Int64 1000\n",
       "  batchsize: Int64 32\n",
       "  decoding: Array{Pair{Union{Regex, String, WebDataset.Renamer}, Function}}((3,))\n",
       "  augmenting: Array{Pair{Union{Regex, String, WebDataset.Renamer}, Function}}((2,))\n",
       "  mapping: #23 (function of type WebDataset.var\"#23#33\")\n",
       "  collating: Array{Pair{Union{Regex, String, WebDataset.Renamer}, Function}}((2,))\n",
       "  csize: Int64 100\n",
       "  ntasks: Int64 4\n",
       "  verbose: Bool false\n",
       "  debug: Bool false\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using WebDataset\n",
    "using Images\n",
    "using Flux: batch\n",
    "\n",
    "function image_augmentation(image)\n",
    "    # generate a fixed size output image for batching here\n",
    "    # here, we just return a mock image for demonstration\n",
    "    result = zeros(Float32, 3, 256, 256)\n",
    "    return result\n",
    "end\n",
    "\n",
    "augmenting = [\n",
    "    # call whatever augmentation you like for individual images\n",
    "    \".img\" => image_augmentation,\n",
    "    # this line ensures that any other sample components are carried along unchanged\n",
    "    \"\" => x->x\n",
    "]\n",
    "\n",
    "collating = [\n",
    "    # .cls is turned  into an Array{Int}\n",
    "    \".cls\" => data->Array{Int}(data),\n",
    "    # RGB images are batched using Flux.batch\n",
    "    \".img\" => l->batch(l),\n",
    "]\n",
    "\n",
    "shards = braceexpand(\"pipe:curl -L -s http://storage.googleapis.com/nvdata-coco/coco-train2014-seg-{000000..000003}.tar\")\n",
    "shards = braceexpand(\"/work-2020/shards/imagenet/imagenet-train-{000000..000146}.tar\")[1:4]\n",
    "\n",
    "desc = DatasetDescriptor(\n",
    "    sources=shards,\n",
    "    shuffle=1000,\n",
    "    batchsize=32,\n",
    "    decoding=default_decoders,\n",
    "    augmenting=augmenting,\n",
    "    collating=collating,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90492f-d238-40f9-8323-548480f3b8e3",
   "metadata": {},
   "source": [
    "Here is a non-threaded loader that's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f82e67e-2ee4-4b43-9016-bafa7dc3f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "if false\n",
    "    ch = debugloader(desc, desc.sources[1:2])\n",
    "    sample = take!(ch)\n",
    "    @show size(sample[\".img\"])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5201111-2d2d-4576-8a62-fef54e7b5a12",
   "metadata": {},
   "source": [
    "This is the multi-threaded loader used for actual training. It loads samples in parallel but delivers them as a simple iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c45e024-4302-499f-8c84-fbc5325be2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count, total) = (1, 32)\n",
      "(count, total) = (101, 3232)\n",
      "(count, total) = (201, 6432)\n",
      "(count, total) = (301, 9632)\n",
      "(count, total) = (401, 12832)\n",
      "(count, total) = (501, 16032)\n",
      "(count, total) = (601, 19232)\n",
      "(count, total) = (701, 22432)\n",
      "(count, total) = (801, 25632)\n",
      "(count, total) = (901, 28832)\n",
      "(count, total) = (1001, 32032)\n",
      " 73.420912 seconds (19.44 M allocations: 121.839 GiB, 2.16% gc time, 0.14% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1052, 33607)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.ntasks = 4\n",
    "(count, total) = (0, 0)\n",
    "global last_sample\n",
    "@time for sample in dataloader(desc)\n",
    "    last_sample = sample\n",
    "    count += 1\n",
    "    total += length(sample[\"__key__\"])\n",
    "    if count % 100 == 1; @show count, total; end\n",
    "end\n",
    "(count, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e14aa-90ff-4d55-a33d-a46ccf73ab28",
   "metadata": {},
   "source": [
    "We are getting a nicely batched sample with additional information about sample keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be33ce6-cf3f-4eed-9494-65365a1af03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"__key__\" => [\"0010656\", \"0937756\", \"0729658\", \"0587807\", \"0307037\", \"0965382\", \"1158354\", \"0432406\", \"0…\n",
       "  \".img\"    => Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]…\n",
       "  \".cls\"    => [8, 732, 568, 457, 240, 753, 904, 337, 430, 53, 462, 818, 436, 554, 609, 577, 1, 540, 981]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cfb8a-584d-4767-99c2-d83c58bb792e",
   "metadata": {},
   "source": [
    "Four Julia threads give us about 450 decoded images per second. The code scales completely linearly, though Julia multithreading seems to have some overhead when running many threads in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ac9480-13da-4b0b-8384-a5def12400df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457.86103542234326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33607/73.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333a012-9111-4217-8316-75d1f35711c8",
   "metadata": {},
   "source": [
    "Exceptions in the multithreaded code are available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd3a474-dee6-4444-abed-08c3464e4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Base.Threads.Atomic{Int64}(0), nothing)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WebDataset.num_exceptions, WebDataset.last_exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a68caa-6d0c-41ca-bde0-aec248a8682c",
   "metadata": {},
   "source": [
    "# Under the Hood\n",
    "\n",
    "The `dataloader` function gives you one particular way of multithreaded data loading and augmentation. It is built from a number of simple iterators that you can reuse and recombine in other ways if you like. You can also use distributed computing for distributed loading and augmentation. The core functionality is expressed by this composition of iterators:\n",
    "\n",
    "```Julia\n",
    "# open either a file or a pipe: source\n",
    "stream = generic_open(source)\n",
    "\n",
    "# iterate over samples in the .tar archive\n",
    "raw = sampleiterator(stream)\n",
    "\n",
    "# shuffle samples with a shuffle buffer\n",
    "shuffled = sampleshuffle(raw, desc.shuffle)\n",
    "\n",
    "# decode the samples based on decoding rules\n",
    "decoded = sampletransforms(shuffled, desc.decoding)\n",
    "\n",
    "# augment the samples based on agumentation rules\n",
    "augmented = sampletransforms(decoded, desc.augmenting)\n",
    "\n",
    "# batch the samples to the given batch size\n",
    "batched = samplebatching(augmented, desc.batchsize)\n",
    "\n",
    "# collate individual fields based on collating rules\n",
    "collated = sampletransforms(batched, desc.collating)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea8176-9ec0-45f5-ab20-8df460e7499c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
